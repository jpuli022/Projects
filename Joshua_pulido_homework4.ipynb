{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# load data\n",
    "mnist= fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y  = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "y_bin = (y == 9).astype(int)  \n",
    "\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minkowski metric order 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     12580\n",
      "           1       0.96      0.94      0.95      1420\n",
      "\n",
      "    accuracy                           0.99     14000\n",
      "   macro avg       0.98      0.97      0.97     14000\n",
      "weighted avg       0.99      0.99      0.99     14000\n",
      "\n",
      "Accuracy: 0.9901428571428571\n",
      "Minkowski metric order 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     12580\n",
      "           1       0.96      0.95      0.96      1420\n",
      "\n",
      "    accuracy                           0.99     14000\n",
      "   macro avg       0.98      0.97      0.97     14000\n",
      "weighted avg       0.99      0.99      0.99     14000\n",
      "\n",
      "Accuracy: 0.9909285714285714\n",
      "Minkowski metric order 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     12580\n",
      "           1       0.96      0.95      0.96      1420\n",
      "\n",
      "    accuracy                           0.99     14000\n",
      "   macro avg       0.98      0.97      0.98     14000\n",
      "weighted avg       0.99      0.99      0.99     14000\n",
      "\n",
      "Accuracy: 0.9911428571428571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# KNN classifier\n",
    "for p in [1, 2, 3]:  \n",
    "    knn = KNeighborsClassifier(n_neighbors=10, p=p, metric='minkowski', n_jobs = -1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    \n",
    "    print(f\"Minkowski metric order {p}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Report:\n",
      "Accuracy: 0.9645714285714285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Classifier Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Report:\n",
      "Accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Classifier Report:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = mnist.data / 255.0  # Normalize data\n",
    "y_all = mnist.target.astype(int)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-Means Clustering: 90.11%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "# K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_train)\n",
    "labels = np.zeros_like(clusters)\n",
    "\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(y_train[mask])[0]\n",
    "accuracy = accuracy_score(y_train_all, labels)\n",
    "print(f\"Accuracy of K-Means Clustering: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN with Minkowski metric of order 1: 95.89%\n",
      "Accuracy of KNN with Minkowski metric of order 2: 96.58%\n",
      "Accuracy of KNN with Minkowski metric of order 3: 96.92%\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "for p in [1, 2, 3]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=10, metric='minkowski', p=p)\n",
    "    knn.fit(X_train_all, y_train_all)\n",
    "    y_pred = knn.predict(X_test_all)\n",
    "    print(f\"Accuracy of KNN with Minkowski metric of order {p}: {accuracy_score(y_test_all, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree: 87.03%\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train_all, y_train_all)\n",
    "y_pred = dt.predict(X_test_all)\n",
    "print(f\"Accuracy of Decision Tree: {accuracy_score(y_test_all, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparing the results from binary and multi-class classifiers on the MNIST dataset, several key differences emerge. Firstly, the binary classifier, focused solely on distinguishing the digit '9' from others, typically shows higher precision and accuracy because of its simpler decision-making process. In contrast, the multi-class classifier, which identifies all digits from 0 to 9, faces more complexity due to the need to manage multiple decision boundaries, often resulting in lower overall accuracy. Furthermore, while the binary model can operate effectively with less diverse training data, the multi-class model requires a balanced dataset to avoid biases and ensure effective learning across all categories. This complexity and data requirement in multi-class classification highlight the trade-offs between specializing in one class versus generalizing across many."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
