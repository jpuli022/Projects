{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af531ec",
   "metadata": {},
   "source": [
    "Building a crawler to analyze a wikipage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beccae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyserini.search.lucene import LuceneSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5821df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"wiki_spider\"\n",
    "    start_urls = ['https://en.wikipedia.org/wiki/Law']\n",
    "\n",
    "    # scrapy settings. scrapy runspider looks for variable called \"custom_settings\" as a dictionary with specific keys\n",
    "    custom_settings = {\n",
    "        'CLOSESPIDER_PAGECOUNT': 100000, # stops running after reaching pagecount value\n",
    "        'CONCURRENT_REQUESTS': 16, # allows the spider to download 16 pages at the same time. This is standard and keeps wiki from blocking the bot\n",
    "        'FEEDS': {'wiki_data2.jsonl': {'format': 'jsonlines', 'overwrite': True}}, # handles the file saving for so i don't have to write open() or write() in the code\n",
    "        'ROBOTSTXT_OBEY': False, # ignores wiki's rules for bots scraping\n",
    "        'DEPTH_PRIORITY': 1, # prioritize shallow links to keep the content as relevant to 'law' as possible\n",
    "        # force first-in-first-out queue to keep the content more relevant\n",
    "        'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue', # store urls in RAM\n",
    "        'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue', # If RAM gets full, Scrapy dumps the overflow URLs onto your Hard Drive\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        # get title\n",
    "        # 'string()' operates like a function and grabs text even if it's trapped inside a <span> tag.\n",
    "        # Ttakes the H1 element and converts the entire tree structure into a single flat string.\n",
    "        # uses universal xpath formula //tag[@attribute=value] so that its applcable to all titles\n",
    "        # html attribute and value for title is id=\"firstHeading\". Use inspect on wiki page to get this\n",
    "        title = response.xpath('string(//h1[@id=\"firstHeading\"])').get()\n",
    "\n",
    "        # get text data\n",
    "        # Grab every <p>, convert to string, join them, and clean whitespace\n",
    "        # div#mw-content-text is the name for the main body of the page\n",
    "        raw_text = [p.xpath('string(.)').get() for p in response.css('div#mw-content-text p')]\n",
    "        text_content = \" \".join(\" \".join(raw_text).split())\n",
    "\n",
    "        # save and set limit to wiki pages with over 500 words\n",
    "        if title and len(text_content) > 500:\n",
    "            yield {\n",
    "                'url': response.url,\n",
    "                'title': title,\n",
    "                'text': text_content\n",
    "            }\n",
    "\n",
    "        # Follow Links\n",
    "        # CSS telling crawler to look inside the body of the wiki page (div#mw-content-text this can be\n",
    "        # found using inspector on the wiki page). this looks for <div>, then identifies id mw-content-text\n",
    "        # a::attr(href) tells it to find the url in the body and go there\n",
    "        for link in response.css('div#mw-content-text a::attr(href)').getall():\n",
    "            if link.startswith('/wiki/') and ':' not in link:\n",
    "                yield response.follow(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "# Creates a folder called \"input\" to store the json file being indexed using pyserini\n",
    "os.makedirs(\"input\", exist_ok=True)\n",
    "\n",
    "# Convert output into pyserini format (url, title, and text)\n",
    "with open(\"wiki_data2.jsonl\", \"r\", encoding=\"utf-8\") as wiki_file, \\\n",
    "     open(\"input/wiki_docs2.jsonl\", \"w\", encoding=\"utf-8\") as dest_file:\n",
    "\n",
    "# Apply to each line in each scrapped wikipidia page\n",
    "    for line in wiki_file:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        # Create Pyserini document\n",
    "        pyserini_doc = {\n",
    "            \"id\": data[\"url\"],            # url acts as unique identifier\n",
    "            \"contents\": data[\"text\"],     # contents to be indexed\n",
    "            \"title\": data[\"title\"]        # title\n",
    "        }\n",
    "\n",
    "        # Write JSON object per line\n",
    "        dest_file.write(json.dumps(pyserini_doc) + \"\\n\")\n",
    "\n",
    "\n",
    "# Build Lucene index\n",
    "command = [\n",
    "    \"python\", \"-m\", \"pyserini.index.lucene\",\n",
    "    \"--collection\", \"JsonCollection\", # input format\n",
    "    \"--input\", \"input\", # directory containing jsonl docs\n",
    "    \"--index\", \"indexes/wiki_index\", # output into indexes/wiki_index folder\n",
    "    \"--generator\", \"DefaultLuceneDocumentGenerator\", \n",
    "    \"--threads\", \"1\",\n",
    "    \"--storePositions\",\n",
    "    \"--storeDocvectors\",\n",
    "    \"--storeRaw\"\n",
    "]\n",
    "\n",
    "# Run commands\n",
    "subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LuceneSearcher\n",
    "searcher = LuceneSearcher(\"indexes/wiki_index\")\n",
    "\n",
    "# State keyword query\n",
    "query = \"constitutional law\"\n",
    "hits = searcher.search(query) # Search for stated query\n",
    "\n",
    "# for each hit, retrieve the stored raw docuemnts, extract the title, and print rank, score, url, and title\n",
    "# loops over each retrived document\n",
    "for i, hit in enumerate(hits):\n",
    "    doc = searcher.doc(hit.docid)\n",
    "\n",
    "    if doc is not None:\n",
    "        doc_json = json.loads(doc.raw()) # load raw doc\n",
    "        title = doc_json.get(\"title\", \"No title\") # extracts title from doc\n",
    "    else:\n",
    "        title = \"No doc found\" # return if no title is found\n",
    "\n",
    "    # Print results\n",
    "    print(\n",
    "        f\"{i+1:<3} \"\n",
    "        f\"Score: {hit.score:.4f} \"\n",
    "        f\"URL: {hit.docid} \"\n",
    "        f\"Title: {title}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define source, input, and index\n",
    "source = \"wiki_data2.jsonl\"\n",
    "input = \"input\"\n",
    "index = \"indexes/wiki_index\"\n",
    "\n",
    "# Creates a folder/ directory called \"input\" to store the json file being indexed using pyserini\n",
    "os.makedirs(input, exist_ok=True)\n",
    "\n",
    "# Document counts to test \n",
    "# Test in increments to measure how indexing time grows as document count increases\n",
    "doc_counts = [100, 500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "# Define variables to store times and lines\n",
    "times = []\n",
    "lines = []\n",
    "\n",
    "# Read source file and store lines in memory\n",
    "with open(source, \"r\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        lines.append(json.loads(line))\n",
    "\n",
    "# Loop through document counts\n",
    "for n in doc_counts:\n",
    "\n",
    "    # Write first n documents to pyserini input file\n",
    "    # Opens a new JSONL file that pyserini will index. File is overwritten for each value of n\n",
    "    with open(os.path.join(input, \"wiki_docs_subset.jsonl\"), \"w\", encoding=\"utf-8\") as fout:\n",
    "        for i in range(n):\n",
    "            data = lines[i]\n",
    "\n",
    "            # convert original wikipedia doc to pyserini format\n",
    "            pyserini_doc = {\n",
    "                \"id\": data[\"url\"],                 # url acts as unique identifier\n",
    "                \"contents\": f\"{data['title']}\\n\\n{data['text']}\",  # title + text combined for indexing\n",
    "                \"title\": data[\"title\"]             # title\n",
    "            }\n",
    "\n",
    "            # writes one JSON object per line\n",
    "            fout.write(json.dumps(pyserini_doc) + \"\\n\")\n",
    "\n",
    "    # Remove previous index if exists\n",
    "    # Indexing starts from scratch\n",
    "    if os.path.exists(index):\n",
    "        subprocess.run([\"rm\", \"-rf\", index])\n",
    "\n",
    "    # Build Lucene index and record time\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"pyserini.index.lucene\",\n",
    "        \"--collection\", \"JsonCollection\",   # input format\n",
    "        \"--input\", input,               # directory containing jsonl docs\n",
    "        \"--index\", index,               # output into indexes/wiki_index folder\n",
    "        \"--generator\", \"DefaultLuceneDocumentGenerator\",\n",
    "        \"--threads\", \"1\",\n",
    "        \"--storePositions\",\n",
    "        \"--storeDocvectors\",\n",
    "        \"--storeRaw\"\n",
    "    ]\n",
    "\n",
    "    # Starts timer before indexing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run indexing commands\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    # Stops timer after indexing finishes\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Append runtime for current document count\n",
    "    times.append(end_time - start_time)\n",
    "    print(f\"Indexed {n} docs in {times[-1]:.2f} seconds\")\n",
    "\n",
    "# Plot runtime vs document count\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(doc_counts, times, marker=\"x\", color=\"red\")\n",
    "plt.title(\"Lucene Indexing Runtime vs Number of Documents\")\n",
    "plt.xlabel(\"Number of Documents\")\n",
    "plt.ylabel(\"Runtime (seconds)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyserini-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
